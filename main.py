# -*- coding: utf-8 -*-
"""PDE example (4).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bhu2M0prME2VtUjbDiaM2pcXmQmSsPyz
"""

# Commented out IPython magic to ensure Python compatibility.
import autograd.numpy as np
from autograd import grad, jacobian
import autograd.numpy.random as npr

from matplotlib import pyplot as plt
from matplotlib import pyplot, cm
from mpl_toolkits.mplot3d import Axes3D
# %matplotlib inline
import autograd.numpy as npa

def get_value(x):
    if type(x) == npa.numpy_boxes.ArrayBox:
        x = x._value
        if type(x) == npa.numpy_boxes.ArrayBox:
            x = get_value(x)
        return x
    else:
        return x

get_value_arr = np.vectorize(get_value)

nx = 10
ny = 10

dx = 1. / nx
dy = 1. / ny

x_space = np.linspace(0, 1, nx)
y_space = np.linspace(0, 1, ny)



from convection_basic import linear_convection_solve

u = np.ones(nx)  # numpy function ones()
u[int(.2 / dx):int(0.5 / dx + 1)] = 2  # setting u = 2 between 0.5 and 1 as per our I.C.s
u,u2D = linear_convection_solve(u, 1.0,dx,dy,1.0,nx, 1.0, ny)
fig, ax = plt.subplots(subplot_kw={"projection": "3d"})
X, Y = np.meshgrid(x_space, y_space)
surf = ax.plot_surface(X, Y, u2D, rstride=1, cstride=1, cmap=cm.viridis,
        linewidth=0, antialiased=False)



def f(x):
    return 0.0 #np.exp(-x[1])

def analytic_solution(x):
    x = get_value(x)

    i = int(x[0]/dy)
    k = int(x[1]/dx)
    if i == u2D.shape[0]:
        i = u2D.shape[0] - 1
    if k == u2D.shape[1]:
        k = u2D.shape[1] - 1

    t = u2D[i][k]
    return t
#    return (1 / (np.exp(np.pi) - np.exp(-np.pi))) * \
#    		np.sin(np.pi * x[0]) * (np.exp(np.pi * x[1]) - np.exp(-np.pi * x[1]))
surface = np.zeros((ny, nx))

for i, x in enumerate(x_space):
    for j, y in enumerate(y_space):
        surface[i][j] = analytic_solution([x, y])

fig, ax = plt.subplots(subplot_kw={"projection": "3d"})
X, Y = np.meshgrid(x_space, y_space)
surf = ax.plot_surface(X, Y, surface, rstride=1, cstride=1, cmap=cm.viridis,
        linewidth=0, antialiased=False)

ax.set_xlim(0, 1)
ax.set_ylim(0, 1)
ax.set_zlim(0, 2)

ax.set_xlabel('$x$')
ax.set_ylabel('$y$');



def sigmoid(x):
    return 1. / (1. + np.exp(-x))

def neural_network(W, x):
    W0 = W[:2,:]
    W1 = W[2].reshape(10,1)
    a1 = sigmoid(np.dot(x, W0))
    return np.dot(a1, W1)


def neural_network_x(x):
    W0 = W[:2,:]
    W1 = W[2].reshape(10,1)
    a1 = sigmoid(np.dot(x, W0))
    return np.dot(a1, W1)


def A(x):
    return analytic_solution(x)


def psy_trial(x, net_out):
    return A(x) + x[0] * (1 - x[0]) * x[1] * (1 - x[1]) * net_out


def loss_function(W, x, y):
    W = get_value(W)
    loss_sum = 0.

    for xi in x:
        for yi in y:

            input_point = np.array([xi, yi])

            net_out = neural_network(W, input_point)[0]

            net_out_jacobian = jacobian(neural_network_x)(input_point)
            net_out_hessian = jacobian(jacobian(neural_network_x))(input_point)

            psy_t = psy_trial(input_point, net_out)
            psy_t_jacobian = jacobian(psy_trial)(input_point, net_out)
            psy_t_hessian = jacobian(jacobian(psy_trial))(input_point, net_out)

            gradient_of_trial_dx = psy_t_jacobian[0]
            gradient_of_trial_dy = psy_t_jacobian[1]
            gradient_of_trial_d2x = psy_t_hessian[0][0]
            gradient_of_trial_d2y = psy_t_hessian[1][1]

            func = f(input_point) # right part function

            err_sqr = ((gradient_of_trial_dx +  gradient_of_trial_dy ) - func)**2
            loss_sum += err_sqr

    return loss_sum

W = [npr.randn(2, 10), npr.randn(10, 1)]
W = npr.randn(3,10);
lmb = 0.001

print(neural_network(W, np.array([1, 1])))

for i in range(100):
    loss_grad =  grad(loss_function)(W, x_space, y_space)
    loss = loss_function(W, x_space, y_space)

    W = W - lmb * loss_grad
    #W[1] = W[1] - lmb * loss_grad[1]
    print(i,loss)



print(loss_function(W, x_space, y_space))

surface2 = np.zeros((ny, nx))
surface = np.zeros((ny, nx))

for i, x in enumerate(x_space):
 for j, y in enumerate(y_space):
     surface[i][j] = analytic_solution(np.array([x, y]))

for i, x in enumerate(x_space):
 for j, y in enumerate(y_space):
     net_outt = neural_network(W, [x, y])[0]
     surface2[i][j] = psy_trial([x, y], net_outt)


print(surface[2])
print(surface2[2])
diff = np.max(np.abs(surface-surface2))
print('difference between analytic and NN ',diff)


fig, ax = plt.subplots(subplot_kw={"projection": "3d"})
X, Y = np.meshgrid(x_space, y_space)
surf = ax.plot_surface(X, Y, surface, rstride=1, cstride=1, cmap=cm.viridis,
       linewidth=0, antialiased=False)

ax.set_xlim(0, 1)
ax.set_ylim(0, 1)
ax.set_zlim(0, 3)

ax.set_xlabel('$x$')
ax.set_ylabel('$y$');
plt.title('Analytic solution')
plt.show()


fig = plt.figure()
ax = fig.gca(projection='3d')
X, Y = np.meshgrid(x_space, y_space)
surf = ax.plot_surface(X, Y, surface2, rstride=1, cstride=1, cmap=cm.viridis,
       linewidth=0, antialiased=False)

ax.set_xlim(0, 1)
ax.set_ylim(0, 1)
ax.set_zlim(0, 3)

ax.set_xlabel('$x$')
ax.set_ylabel('$y$');
plt.title('NN solution')
plt.show()

print(loss_function(W, x_space, y_space))

